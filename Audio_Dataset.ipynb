{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory monitor\n",
    "from my_memory_profiler import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calcular Extension</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_extension(data_dir, sample_rate, time_leng, overlap):\n",
    "\n",
    "    def label_generator(data_dir):\n",
    "        data_list = []\n",
    "        \n",
    "        for dirname, _, filenames in os.walk(data_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.wav'):  \n",
    "                    identifiers = filename.split('.')[0].split('_')\n",
    "                    Speaker_ID = int(identifiers[-1])\n",
    "                    Label = 1 if identifiers[0] == 'PD' else 0\n",
    "                    data_list.append({\n",
    "                        \"Speaker_ID\": Speaker_ID,\n",
    "                        \"Label\": Label,\n",
    "                        \"FileName\": filename\n",
    "                    })\n",
    "        \n",
    "        audio_labels = pd.DataFrame(data_list)\n",
    "            \n",
    "        return audio_labels\n",
    "    \n",
    "    def num_seg(data_dir, audio_labels, SAMPLE_RATE, time_leng, overloap, output_csv=None, filenames=False):\n",
    "        sample_leng=int(time_leng*SAMPLE_RATE)\n",
    "        segments_info=[]\n",
    "\n",
    "        for data_ind, audio_label in enumerate(audio_labels.values):\n",
    "            file_name=audio_label[-1]\n",
    "            data_path=os.path.join(data_dir, file_name)\n",
    "            audio, sample_rate = librosa.load(data_path, sr=SAMPLE_RATE)\n",
    "            audio_len=len(audio)\n",
    "            audio=audio/np.max(abs(audio))\n",
    "            indx=[i for i,x in enumerate(np.sqrt(abs(audio))) if x>.30]\n",
    "            segments=0\n",
    "            if (indx[0]+sample_leng)<audio_len:\n",
    "                for i in range(int((-indx[0]+indx[len(indx)-1])/(sample_leng/overloap))):\n",
    "                    ind_start = i * int(sample_leng/overloap)+indx[0]\n",
    "                    ind_end = ind_start + sample_leng\n",
    "                    if ind_end <= indx[-1]:\n",
    "                        segments+=1\n",
    "                if filenames:\n",
    "                    segments_info.append({\n",
    "                        'Filename': file_name,\n",
    "                        'Segments': segments})\n",
    "                else: \n",
    "                    segments_info.append({'Segments':segments})\n",
    "                \n",
    "                print(\" Processed {}/{} files\".format(data_ind,2902),end='\\n')\n",
    "            else:\n",
    "                print(\" Processed {}/{} files\".format(data_ind,2902),end='\\n')\n",
    "                if filenames:\n",
    "                    segments_info.append({\n",
    "                        'Filename': file_name,\n",
    "                        'Segments': segments})\n",
    "                else: \n",
    "                    segments_info.append({'Segments':segments})\n",
    "        \n",
    "        df_segments = pd.DataFrame(segments_info)\n",
    "        if(output_csv is None):\n",
    "            return df_segments\n",
    "        else:\n",
    "            df_segments.to_csv(output_csv, index=False)\n",
    "    \n",
    "    labels=label_generator(data_dir)\n",
    "    segments=num_seg(data_dir, labels, sample_rate, time_leng, overlap)\n",
    "    extension_DB=pd.concat([labels, segments], axis=1)\n",
    "    extension_DB.to_csv('extension.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Funciones de preprocesado</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_select_signals(data_path, sample_rate):\n",
    "    segment_length=0.4\n",
    "    overlap_factor=2\n",
    "    amplitude_threshold=0.30\n",
    "    audio, _ = librosa.load(data_path, sr=sample_rate)\n",
    "    audio_len = len(audio)\n",
    "    audio = audio / np.max(np.abs(audio)) \n",
    "\n",
    "    time_length = segment_length\n",
    "    sample_length = int(time_length * sample_rate)\n",
    "    overlap = overlap_factor\n",
    "\n",
    "    signals = []\n",
    "    y_label = []\n",
    "    subject_group = []\n",
    "\n",
    "    indx = [i for i, x in enumerate(np.sqrt(np.abs(audio))) if x > amplitude_threshold]\n",
    "\n",
    "    segments = 0\n",
    "    if len(indx) > 0 and (indx[0] + sample_length) < audio_len:\n",
    "        for i in range(int((-indx[0] + indx[-1]) / (sample_length / overlap))):\n",
    "            ind_start = i * int(sample_length / overlap) + indx[0]\n",
    "            ind_end = ind_start + sample_length\n",
    "            if ind_end <= indx[-1]:\n",
    "                signal = np.zeros(sample_length)\n",
    "                signal = audio[ind_start:int(ind_end)]\n",
    "                signals.append(signal)\n",
    "                y_label.append('Label') \n",
    "                subject_group.append('Speaker_ID')  \n",
    "                segments += 1\n",
    "\n",
    "    signals = np.stack(signals, axis=0) if signals else np.empty((0, sample_length))\n",
    "    y_label = np.array(y_label)\n",
    "    subject_group = np.array(subject_group)\n",
    "\n",
    "    return signals\n",
    "\n",
    "def to_spectrogram(signal, sample_rate):\n",
    "    n_fft = 2048\n",
    "    win_length = int(0.015*sample_rate) \n",
    "    hop_length = int(0.010*sample_rate)\n",
    "    n_mels = 65 \n",
    "\n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    mel_spect = librosa.power_to_db(mel_spectrogram(torch.from_numpy(signal)))\n",
    "    mel_spect_norm=scaler.fit_transform(mel_spect)\n",
    "    \n",
    "    return mel_spect_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Definición del Dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAudioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, sample_rate, to_spectrogram=False, extension_path=None):\n",
    "        \n",
    "        if extension_path is None:\n",
    "            calculate_extension(data_dir)\n",
    "        else:\n",
    "            extension=pd.read_csv(extension_path)\n",
    "\n",
    "        self.labels=extension['Label'].values\n",
    "        self.IDs=extension['Speaker_ID'].values\n",
    "        self.filenames=extension['FileName'].values\n",
    "        self.num_segs=extension['Segments'].values\n",
    "        self.spec_id_to_file_id = np.concatenate([np.full(count, idx) for idx, count in enumerate(self.num_segs)])\n",
    "\n",
    "        # sample rate y path al directorio\n",
    "        self.sample_rate=sample_rate\n",
    "        self.data_dir=data_dir\n",
    "\n",
    "        # to_spectrogram: True or False\n",
    "        self.to_spectrogram=to_spectrogram\n",
    "\n",
    "        # signal index\n",
    "        signal_index=[]\n",
    "        for number in self.num_segs:\n",
    "            for i in range(number):\n",
    "                signal_index.append(i)\n",
    "\n",
    "        self.signal_index=signal_index\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        # numero de segmentos/espectrogramas que puede obtener getitem\n",
    "        return len(self.spec_id_to_file_id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # mapeo de indices: file_index es el que se usa para hacer el fetch al DB. \n",
    "        file_index=self.spec_id_to_file_id[idx]\n",
    "\n",
    "        ### if(file_index in self.isincache): \n",
    "\n",
    "        # data_path = directory/path + /filename.wav\n",
    "        data_path=os.path.join(self.data_dir, self.filenames[file_index])\n",
    "\n",
    "        print(f\"fetched segment {self.signal_index[idx]} of audio {self.filenames[file_index]}\")\n",
    "\n",
    "        # audio, sample_rate = librosa.load(data_path, sr=SAMPLE_RATE)\n",
    "        label=self.labels[file_index]\n",
    "        subject_group= self.IDs[file_index]\n",
    "\n",
    "        signals=process_select_signals(data_path, self.sample_rate)\n",
    "        \n",
    "        if(self.to_spectrogram):\n",
    "            audio=to_spectrogram(signals[self.signal_index[idx]], self.sample_rate)\n",
    "            ### self.cache = [to_spectrogram(sig) for sig in signals_cache]\n",
    "        else:\n",
    "            audio=signals[idx]\n",
    "            ### self.cache=signals\n",
    "        \n",
    "        return audio, label, subject_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pruebas inciales</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtención de la extensión si no se dispone de ella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_NeuroV = 'neurovoz/zenodo_upload/audios/'\n",
    "SAMPLE_RATE=44100\n",
    "calculate_extension(DATA_PATH_NeuroV, SAMPLE_RATE, time_leng=0.4, overlap=2) # genera extension.csv en el cwd\n",
    "extension_DB=pd.read_csv('extension.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Demostración de integridad de labels y sujetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE=extension_DB['Segments'].sum()\n",
    "BATCH_SIZE=9\n",
    "\n",
    "# Dataset\n",
    "datasetAudio = CustomAudioDataset(extension_DB, DATA_PATH_NeuroV, SAMPLE_RATE, to_spectrogram=True)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(datasetAudio, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "spectrogram_batch, label_batch, subject_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1,BATCH_SIZE+1):\n",
    "    sample_idx= i-1\n",
    "    img = spectrogram_batch[sample_idx]\n",
    "    label=label_batch[sample_idx]\n",
    "    subject=subject_batch[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    if label:\n",
    "        title='PD'\n",
    "    else:\n",
    "        title='HC'\n",
    "    plt.title('{}, ID:{}'.format(title,subject))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Ejecución dentro de un bucle de entrenamiento</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "iters=int(DATASET_SIZE/BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(datasetAudio, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    for i in range(iters):\n",
    "\n",
    "        spectrogram_batch, label_batch, subject_batch = next(iter(dataloader))\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
