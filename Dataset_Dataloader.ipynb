{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NUM_SEGMENTOS Y LABEL_GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_select_signals(data_path, sample_rate):\n",
    "    segment_length=0.4\n",
    "    overlap_factor=2\n",
    "    amplitude_threshold=0.30\n",
    "    audio, _ = librosa.load(data_path, sr=sample_rate)\n",
    "    audio_len = len(audio)\n",
    "    audio = audio / np.max(np.abs(audio))  # Normalize audio\n",
    "\n",
    "    time_length = segment_length\n",
    "    sample_length = int(time_length * sample_rate)\n",
    "    overlap = overlap_factor\n",
    "\n",
    "    signals = []\n",
    "    y_label = []\n",
    "    subject_group = []\n",
    "\n",
    "    # Find segments\n",
    "    indx = [i for i, x in enumerate(np.sqrt(np.abs(audio))) if x > amplitude_threshold]\n",
    "\n",
    "    segments = 0\n",
    "    if len(indx) > 0 and (indx[0] + sample_length) < audio_len:\n",
    "        for i in range(int((-indx[0] + indx[-1]) / (sample_length / overlap))):\n",
    "            ind_start = i * int(sample_length / overlap) + indx[0]\n",
    "            ind_end = ind_start + sample_length\n",
    "            if ind_end <= indx[-1]:\n",
    "                signal = np.zeros(sample_length)\n",
    "                signal = audio[ind_start:int(ind_end)]\n",
    "                signals.append(signal)\n",
    "                y_label.append('Label')  # Replace 'Label' with actual label assignment logic\n",
    "                subject_group.append('Speaker_ID')  # Replace 'Speaker_ID' with actual speaker ID assignment logic\n",
    "                segments += 1\n",
    "\n",
    "        print(f\"Processed audio file: {data_path}\")\n",
    "        print(f\"Time audio: {(audio_len - 1) / sample_rate} seconds, Segments: {segments}\")\n",
    "    else:\n",
    "        print(f\"No segments found in audio file: {data_path}\")\n",
    "\n",
    "    signals = np.stack(signals, axis=0) if signals else np.empty((0, sample_length))\n",
    "    y_label = np.array(y_label)\n",
    "    subject_group = np.array(subject_group)\n",
    "\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_spectrogram(signal, sample_rate):\n",
    "    n_fft = 2048\n",
    "    win_length = int(0.015*sample_rate) \n",
    "    hop_length = int(0.010*sample_rate)\n",
    "    n_mels = 65 \n",
    "\n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        onesided=True,\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    mel_spect = librosa.power_to_db(mel_spectrogram(torch.from_numpy(signal)))\n",
    "    mel_spect_norm=scaler.fit_transform(mel_spect)\n",
    "    \n",
    "    return mel_spect_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params: audio_labels: list of\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_labels, num_segs,  data_dir, sample_rate, to_spectrogram):\n",
    "        \n",
    "        # primer DF: labels, IDS, y nombre de los ficheros \n",
    "        label_generator=pd.read_csv(audio_labels)\n",
    "        self.audio_labels=label_generator['Label'].values() # se obtiene de un script aparte y contiene [Speaker_ID, label, filename]\n",
    "        self.audio_IDs=label_generator['Speaker_ID'].values()\n",
    "        self.filenames=label_generator['FileName'].values() # comprobar esto\n",
    "\n",
    "        # segundo DF: numero de segmentos y mapeo de indices\n",
    "        num_segmentos=pd.read_csv(num_segs)\n",
    "        self.num_segs=num_segmentos['Segments'].values()\n",
    "        self.spec_id_to_file_id = np.concatenate([np.full(count, idx) for idx, count in enumerate(self.num_segs)])\n",
    "\n",
    "        # sample rate y path al directorio\n",
    "        self.sample_rate=sample_rate\n",
    "        self.data_dir=data_dir\n",
    "\n",
    "        # to_spectrogram: True or False\n",
    "        self.to_spectrogram=to_spectrogram\n",
    "\n",
    "        # cache\n",
    "        # self.isincache=[]\n",
    "        # self.cache=[]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        # numero de segmentos/espectrogramas que puede obtener getitem\n",
    "        return len(self.spec_id_to_file_id) ### tambien seria sum(self.num_segs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # mapeo de indices: file_index es el que se usa para hacer el fetch al DB. \n",
    "        file_index=self.spec_id_to_file_id[idx]\n",
    "\n",
    "        ### if(file_index in self.isincache): \n",
    "\n",
    "        # data_path = directory/path + /filename.wav\n",
    "        data_path=os.path.join(self.data_dir, self.audio_labels[file_index, -1]) \n",
    "\n",
    "        # audio, sample_rate = librosa.load(data_path, sr=SAMPLE_RATE)\n",
    "        label=self.audio_labels[file_index]\n",
    "        subject_group= self.audio_IDs[file_index]\n",
    "\n",
    "        signals=process_select_signals(data_path, self.sample_rate)\n",
    "        \n",
    "        if(self.to_spectrogram):\n",
    "            audio=to_spectrogram(audio, self.sample_rate)\n",
    "            ### self.cache = [to_spectrogram(sig) for sig in signals_cache]\n",
    "        else:\n",
    "            audio=signals[idx]\n",
    "            ### self.cache=signals\n",
    "        \n",
    "        return audio, label, subject_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE=44100\n",
    "DATA_SIZE=pd.read_csv('prueba.csv')['Segments'].sum()\n",
    "DATA_PATH_NeuroV = 'neurovoz/zenodo_upload/audios/'\n",
    "labels='labels.csv'\n",
    "\n",
    "#Dataset\n",
    "datasetAudio = CustomAudioDataset(labels, DATA_PATH_NeuroV)\n",
    "\n",
    "#DataLoader\n",
    "dataloader = DataLoader(datasetAudio, batch_size=32, shuffle=True)\n",
    "\n",
    "iters=int(DATASET_SIZE/BATCH_SIZE)\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    for i in range(iters):\n",
    "\n",
    "        x_batch, y_batch, subject_batch, cache_batch =next(iter(dataloader))\n",
    "\n",
    "        model.train() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
