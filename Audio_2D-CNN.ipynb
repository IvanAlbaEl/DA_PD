{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.autograd import Function\n",
    "from imblearn.metrics import sensitivity_specificity_support\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_dim, kernel_size_1=6, kernel_size_2=9, depth_CL=64, neurons_MLP=32, drop_out=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # img_dim = (65, 41)\n",
    "        height, width = img_dim\n",
    "\n",
    "        padding=1\n",
    "        dilation=1\n",
    "        stride=1\n",
    "\n",
    "        # Calculo de las dimensiones de la salida de la primera capa convolucional\n",
    "            # En el codigo del paper pone: math.ceil((math.ceil((65-kernel_size_1)/2+1)-kernel_size_2)/2+1)\n",
    "            # En el codigo del paper pone: math.ceil((math.ceil((41-kernel_size_1)/2+1)-kernel_size_2)/2+1) \n",
    "            # En conv2d pytorch documentation pone: out_height = floor((height + 2*padding - dilation*(kernel_size_1-1) - 1) / stride + 1)\n",
    "            # En conv2d pytorch documentation pone: out_width = floor((width + 2*padding - dilation*(kernel_size_1-1) - 1) / stride + 1)\n",
    "        Hout1 = floor((height + 2*padding - dilation*(kernel_size_1-1) - 1) / stride + 1)\n",
    "        Wout1 = floor((width + 2*padding - dilation*(kernel_size_1-1) - 1) / stride + 1)\n",
    "\n",
    "        # Pooling\n",
    "        Hout1p = floor(Hout1/2)\n",
    "        Wout1p = floor(Wout1/2)\n",
    "\n",
    "        # Calculo de las dimensiones de la salida de la segunda capa convolucional\n",
    "\n",
    "        Hout2 = floor((Hout1p + 2*padding - dilation*(kernel_size_2-1) - 1) / stride + 1)\n",
    "        Wout2 = floor((Wout1p + 2*padding - dilation*(kernel_size_2-1) - 1) / stride + 1)\n",
    "\n",
    "        # Pooling\n",
    "        Hout2p = floor(Hout2/2)\n",
    "        Wout2p = floor(Wout2/2)\n",
    "        \n",
    "        input_size = depth_CL*Hout2p*Wout2p\n",
    "\n",
    "        # 1. Convolutional Layer 1\n",
    "        self.cl1=nn.Sequential(\n",
    "            # bias = False porque se aplica BatchNorm2d inmmediatamente despues de la convolucion\n",
    "            nn.Conv2d(in_channels=1, out_channels=depth_CL, kernel_size=kernel_size_1, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(depth_CL),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(p=drop_out),\n",
    "        )\n",
    "\n",
    "        # 2. Convolutional Layer 2\n",
    "        self.cl2=nn.Sequential(\n",
    "            # bias = False porque se aplica BatchNorm2d inmmediatamente despues de la convolucion\n",
    "            nn.Conv2d(in_channels=depth_CL, out_channels=depth_CL, kernel_size=kernel_size_2, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(depth_CL),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(p=drop_out),\n",
    "        )\n",
    "\n",
    "        # 3. Fully Connected Layer 1\n",
    "        self.fcl1=nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=neurons_MLP),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=drop_out),\n",
    "        )\n",
    "\n",
    "        # 4. Fully Connected Layer 2\n",
    "        self.fcl2=nn.Sequential(\n",
    "            nn.Linear(in_features=neurons_MLP),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=drop_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_cl1=self.cl1(x)\n",
    "        y_cl2=self.cl2(y_cl1)\n",
    "        y_cl2_flat=torch.flatten(y_cl2, start_dim=1)\n",
    "        y_fcl1=self.fcl1(y_cl2_flat)\n",
    "        y_fcl2=self.fcl2(y_fcl1)\n",
    "        return y_cl1, y_cl2, y_cl2_flat, y_fcl1, y_fcl2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def reset_weights(m):\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "model = CNN_model()\n",
    "model.apply(reset_weights)\n",
    "\n",
    "\n",
    "model.train()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
